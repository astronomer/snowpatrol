{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snowflake.snowpark import (\n",
    "    Session,\n",
    "    functions as F, \n",
    "    types as T\n",
    ")\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import calendar\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas.tseries.offsets import MonthEnd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from sklearn.ensemble import IsolationForest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "connection_params = json.loads(os.environ[\"AIRFLOW_CONN_SNOWFLAKE_ADMIN\"])\n",
    "connection_params['user'] = connection_params.pop('login')\n",
    "connection_params['account'] = connection_params['extra'].pop('account')\n",
    "connection_params['database'] = connection_params['extra'].pop('database')\n",
    "connection_params['schema'] = connection_params['schema']\n",
    "connection_params['region'] = connection_params['extra'].pop('region')\n",
    "connection_params['application'] = connection_params['extra'].pop('application')\n",
    "snowpark_session = Session.builder.configs(connection_params).create()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "account_start_cutoff_date = datetime.date(2022, 11, 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data engineering\n",
    "currency_usage = snowpark_session.table('SNOWFLAKE.ORGANIZATION_USAGE.USAGE_IN_CURRENCY_DAILY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature engineering\n",
    "usage = currency_usage.filter((F.col('USAGE_DATE') >= account_start_cutoff_date) &\n",
    "                              (F.col('USAGE_DATE') < datetime.date.today()))\n",
    "                                         \n",
    "pivot_values = usage.select('USAGE_TYPE').distinct().to_pandas().USAGE_TYPE.to_list()\n",
    "usage_df = usage.select('USAGE_DATE', 'USAGE_TYPE', 'USAGE')\\\n",
    "                         .pivot(pivot_col='USAGE_TYPE', values=pivot_values)\\\n",
    "                         .sum('USAGE')\\\n",
    "                         .sort('USAGE_DATE')\\\n",
    "                         .to_pandas()\\\n",
    "\n",
    "usage_df.columns = ['date']+pivot_values\n",
    "usage_df.date = pd.to_datetime(usage_df.date)\n",
    "usage_df.set_index('date', inplace=True)\n",
    "usage_df.fillna(value=0, inplace=True)\n",
    "usage_df = usage_df.apply(pd.to_numeric, downcast='float')\n",
    "usage_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rc('figure',figsize=(12,6))\n",
    "plt.rc('font',size=15)\n",
    "\n",
    "fig, axs = plt.subplots(2, 1, sharex='all', layout='constrained')\n",
    "axs[0].plot(usage_df['compute'])\n",
    "# axs[0].set_xlabel('Date')\n",
    "axs[0].set_ylabel('Dollars')\n",
    "axs[0].grid(True)\n",
    "\n",
    "axs[1].plot(usage_df['storage'])\n",
    "axs[1].set_xlabel('Date')\n",
    "axs[1].set_ylabel('Dollars')\n",
    "axs[1].grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_stl = seasonal_decompose(usage_df.compute, model='additive')\n",
    "\n",
    "fig = compute_stl.plot()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_trend = compute_stl.trend.fillna(0).values.reshape(-1,1)\n",
    "compute_stationary = usage_df.compute.values.reshape(-1, 1) - compute_trend\n",
    "\n",
    "compute_scaled = StandardScaler().fit_transform(compute_stationary)\n",
    "\n",
    "compute_model =  IsolationForest()\n",
    "compute_model.fit(compute_scaled)\n",
    "usage_df['compute_anomaly'] = compute_model.score_samples(compute_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,6))\n",
    "\n",
    "anomalies = usage_df.loc[(usage_df.compute_anomaly <= -0.65) & (usage_df.compute > usage_df.compute.mean()), ['compute']]\n",
    "anomalies\n",
    "\n",
    "ax.plot(usage_df.index, usage_df.compute, color='black', label = 'Normal')\n",
    "ax.scatter(anomalies.index, anomalies.compute, color='red', label = 'Anomaly')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "storage_stl = seasonal_decompose(usage_df.storage, model='additive')\n",
    "fig = storage_stl.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "storage_trend = storage_stl.trend.fillna(0).values.reshape(-1,1)\n",
    "storage_stationary = usage_df.storage.values.reshape(-1, 1) - storage_trend\n",
    "\n",
    "storage_scaled = StandardScaler().fit_transform(storage_stationary)\n",
    "\n",
    "storage_model =  IsolationForest()\n",
    "storage_model.fit(storage_scaled)\n",
    "usage_df['storage_anomaly'] = storage_model.score_samples(storage_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,6))\n",
    "\n",
    "anomalies = usage_df.loc[(usage_df.storage_anomaly <= -0.65) & (usage_df.storage > usage_df.storage.mean()), ['storage']]\n",
    "anomalies\n",
    "\n",
    "ax.plot(usage_df.index, usage_df.storage, color='black', label = 'Normal')\n",
    "ax.scatter(anomalies.index, anomalies.storage, color='red', label = 'Anomaly')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "usage_df = usage.select('USAGE_DATE', 'USAGE_TYPE', 'USAGE')\\\n",
    "                         .pivot(pivot_col='USAGE_TYPE', values=pivot_values)\\\n",
    "                         .sum('USAGE')\\\n",
    "                         .sort('USAGE_DATE')\\\n",
    "                         .to_pandas()\\\n",
    "\n",
    "usage_df.columns = ['date']+pivot_values\n",
    "usage_df.date = pd.to_datetime(usage_df.date)\n",
    "usage_df.set_index('date', inplace=True)\n",
    "usage_df.fillna(value=0, inplace=True)\n",
    "usage_df = usage_df.apply(pd.to_numeric, downcast='float')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import Normalizer\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "\n",
    "scalar = StandardScaler()\n",
    "data = scalar.fit_transform(usage_df)\n",
    "\n",
    "ds = tf.keras.preprocessing.timeseries_dataset_from_array(\n",
    "     data=data,\n",
    "     targets=data,\n",
    "     sequence_length=7)\n",
    "\n",
    "X, y = next(iter(ds))\n",
    "\n",
    "model = keras.Sequential(\n",
    "    [\n",
    "        keras.layers.LSTM(\n",
    "            units=64, \n",
    "            kernel_initializer='he_uniform', \n",
    "            batch_input_shape=(None, X.shape[1], X.shape[2]), \n",
    "            return_sequences=True, \n",
    "            # activation='relu',\n",
    "            name='enc1'),\n",
    "        keras.layers.LSTM(\n",
    "            units=32, \n",
    "            kernel_initializer='he_uniform', \n",
    "            return_sequences=True, \n",
    "            # activation='relu',\n",
    "            name='enc2'),\n",
    "        keras.layers.LSTM(\n",
    "            units=16, \n",
    "            kernel_initializer='he_uniform', \n",
    "            return_sequences=False, \n",
    "            # activation='relu',\n",
    "            name='enc3'),\n",
    "        keras.layers.RepeatVector(\n",
    "            n=7, \n",
    "            name='encoder_decoder_bridge'),\n",
    "        keras.layers.LSTM(\n",
    "            units=16, \n",
    "            kernel_initializer='he_uniform', \n",
    "            return_sequences=True, \n",
    "            # activation='relu',\n",
    "            name='dec1'),\n",
    "        keras.layers.LSTM(\n",
    "            units=32, \n",
    "            kernel_initializer='he_uniform', \n",
    "            return_sequences=True, \n",
    "            # activation='relu',\n",
    "            name='dec2'),\n",
    "        keras.layers.LSTM(\n",
    "            units=64, \n",
    "            kernel_initializer='he_uniform', \n",
    "            return_sequences=True, \n",
    "            # activation='relu',\n",
    "            name='dec3'),\n",
    "        keras.layers.TimeDistributed(layer=keras.layers.Dense(X.shape[2]))\n",
    "    ]\n",
    ")\n",
    "\n",
    "model.compile(loss=\"mse\", optimizer=tf.keras.optimizers.legacy.Adam()) #(learning_rate=0.001))\n",
    "model.build()\n",
    "# print(model.summary())\n",
    "\n",
    "history = model.fit(\n",
    "    x=X,\n",
    "    y=X,\n",
    "    epochs=200,\n",
    "    batch_size=128,\n",
    "    validation_split=0.1,\n",
    "    callbacks=[\n",
    "        keras.callbacks.EarlyStopping(monitor=\"val_loss\", \n",
    "                                      patience=20, \n",
    "                                      mode=\"min\")\n",
    "    ],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history[\"loss\"], label=\"Training Loss\")\n",
    "plt.plot(history.history[\"val_loss\"], label=\"Validation Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "X_pred = model.predict(X)\n",
    "mse = np.mean(np.power(X - X_pred, 2), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_df = pd.DataFrame(scalar.inverse_transform(mse), columns=usage_df.columns)\n",
    "pred_df = pd.DataFrame(mse, columns=usage_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df.loc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#normal day\n",
    "usage_df.iloc[-4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(pred_df.compute, label=\"Compute\")\n",
    "plt.plot(pred_df.storage, label=\"Storage\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "snowcast",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
