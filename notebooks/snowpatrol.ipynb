{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import snowflake.connector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the following environment variable to connect to Snowflake:\n",
    "\n",
    "```shell\n",
    "export AIRFLOW_CONN_SNOWFLAKE_CONN='{\n",
    "    \"conn_type\": \"snowflake\",\n",
    "    \"login\": \"<your-username>\",\n",
    "    \"password\": \"<your-password>\",\n",
    "    \"schema\": \"ORGANIZATION_USAGE\",\n",
    "    \"extra\": {\n",
    "        \"account\": \"<your-account>\",\n",
    "        \"database\": \"SNOWFLAKE\",\n",
    "        \"warehouse\": \"<your-warehouse>\"\n",
    "    }\n",
    "}'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect to Snowflake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "airflow_con = json.loads(os.environ[\"AIRFLOW_CONN_SNOWFLAKE_CONN\"])\n",
    "snowflake_params = {\n",
    "    \"user\": airflow_con[\"login\"],\n",
    "    \"password\": airflow_con[\"password\"],\n",
    "    \"account\": airflow_con[\"extra\"][\"account\"],\n",
    "    \"region\": airflow_con[\"extra\"][\"region\"],\n",
    "    \"warehouse\": airflow_con[\"extra\"][\"warehouse\"],\n",
    "    \"database\": airflow_con[\"extra\"][\"database\"],\n",
    "    \"schema\": airflow_con[\"schema\"],\n",
    "}\n",
    "conn = snowflake.connector.connect(**snowflake_params)\n",
    "cursor = conn.cursor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform EDA on the available Snowflake cost and usage tables\n",
    "\n",
    "Snowflake pricing tier depends on a few different variables such as the cloud provider, region, plan type, services, and more. \n",
    "Snowflake costs are grouped into two distinct categories: **storage** and **compute.** \n",
    "\n",
    "- **Storage**: The monthly cost for storing data in Snowflake is based on a flat rate per terabyte (TB). Storage is calculated monthly based on the average number of on-disk bytes stored each day. The flat rate is determined based on the type of account (Capacity or On Demand) and the region (US / EU).\n",
    "\n",
    "- **Compute:** Compute costs represent credits used for:\n",
    "\n",
    "    - **Virtual** **Warehouse**: Credits are charged based on the number of virtual warehouses you use, how long they run, and their size.\n",
    "\n",
    "    - **Cloud Services**: Cloud Services coordinate activities across Snowflake. Credits are consumed as Snowflake performs behind-the-scenes tasks such as authentication, infrastructure management, metadata management, query parsing and optimization, access control. Charged only if the daily consumption of cloud services resources exceeds 10% of the daily warehouse usage\n",
    "\n",
    "    - **Serverless features**:  Services such as Snowpipe, Snowpipe streaming, Database Replication, Materialized Views, Automatic Clustering and Search Optimization also consume Snowflake credits when they are used. \n",
    "\n",
    "For all the details, you can read Snowflakeâ€™s documentation here: https://docs.snowflake.com/en/user-guide/cost-understanding-overall and here https://www.snowflake.com/pricing/pricing-guide/\n",
    "\n",
    "\n",
    "#### Relevant Snowflake Tables\n",
    "Pricing information can be fetched from the `USAGE_IN_CURRENCY_DAILY` and `WAREHOUSE_METERING_HISTORY` views in the ORGANIZATION_USAGE schema.\n",
    "\n",
    "Astronomer uses the legacy usage view which does not provide meaningful compute usage breakdown. For that reason we need to manually aggregate the various usage types into the main cost categories: compute (and its subcategories), data transfer, storage and other. These categories follow Snowflake's definitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "currency_query = \"\"\"\n",
    "SELECT ORGANIZATION_NAME, \n",
    "    CONTRACT_NUMBER, \n",
    "    ACCOUNT_NAME, \n",
    "    ACCOUNT_LOCATOR, \n",
    "    REGION, \n",
    "    SERVICE_LEVEL, \n",
    "    USAGE_DATE, \n",
    "    USAGE_TYPE, \n",
    "    CURRENCY, \n",
    "    BALANCE_SOURCE,\n",
    "    CASE\n",
    "      WHEN USAGE_TYPE LIKE '%storage'              THEN 'storage'\n",
    "      WHEN USAGE_TYPE LIKE '%compute'              THEN 'compute'\n",
    "      WHEN USAGE_TYPE LIKE '%cloud services'       THEN 'compute'  \n",
    "      WHEN USAGE_TYPE LIKE '%automatic clustering' THEN 'compute' \n",
    "      WHEN USAGE_TYPE LIKE 'serverless tasks'      THEN 'compute' \n",
    "      WHEN USAGE_TYPE LIKE '%snowpipe%'            THEN 'compute' \n",
    "      WHEN USAGE_TYPE LIKE '%snowpark%'            THEN 'compute'\n",
    "      WHEN USAGE_TYPE LIKE '%materialized views'   THEN 'compute'\n",
    "      WHEN USAGE_TYPE LIKE '%data transfer'        THEN 'data transfer'\n",
    "      WHEN USAGE_TYPE LIKE '%support%'             THEN 'other' \n",
    "      ELSE ''\n",
    "    END AS RATING_TYPE,\n",
    "    CASE\n",
    "      WHEN USAGE_TYPE LIKE '%storage'              THEN 'storage'\n",
    "      WHEN USAGE_TYPE LIKE '%compute'              THEN 'virtual warehouse'\n",
    "      WHEN USAGE_TYPE LIKE '%cloud services'       THEN 'cloud services'  \n",
    "      WHEN USAGE_TYPE LIKE '%automatic clustering' THEN 'serverless' \n",
    "      WHEN USAGE_TYPE LIKE 'serverless tasks'      THEN 'serverless' \n",
    "      WHEN USAGE_TYPE LIKE '%snowpipe%'            THEN 'serverless' \n",
    "      WHEN USAGE_TYPE LIKE '%snowpark%'            THEN 'serverless'\n",
    "      WHEN USAGE_TYPE LIKE '%materialized views'   THEN 'serverless'\n",
    "      WHEN USAGE_TYPE LIKE '%data transfer'        THEN 'data transfer'\n",
    "      WHEN USAGE_TYPE LIKE '%support%'             THEN 'other' \n",
    "      ELSE ''\n",
    "    END AS RESOURCE_TYPE,\n",
    "    USAGE_IN_CURRENCY\n",
    "FROM SNOWFLAKE.ORGANIZATION_USAGE.USAGE_IN_CURRENCY_DAILY\n",
    "WHERE USAGE_DATE BETWEEN '2020-06-01' AND '2023-12-31'\n",
    "AND ACCOUNT_NAME = 'AA12345' -- Account number\n",
    "\"\"\"\n",
    "\n",
    "cursor.execute(currency_query)\n",
    "usage_df = cursor.fetch_pandas_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.float_format\", lambda x: \"%.2f\" % x)\n",
    "\n",
    "usage_df.groupby(\n",
    "    by=[usage_df[\"USAGE_DATE\"].map(lambda x: x.year), \"RATING_TYPE\", \"RESOURCE_TYPE\"]\n",
    ").agg({\"USAGE_IN_CURRENCY\": \"sum\"}).unstack(level=[1, 2], fill_value=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Key insights from this table:\n",
    "- 99% of costs comes from compute\n",
    "- 1% of costs from data storage\n",
    "- Astronomer never incurred data transfer costs (no data egress).\n",
    "- At 0.02% serverless costs are negligible\n",
    "- Within the Compute category, 7.76% comes from Cloud Services, the rest comes from running Virtual Warehouses (91.24%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.line(\n",
    "    usage_df.sort_values(by=\"USAGE_DATE\"),\n",
    "    x=\"USAGE_DATE\",\n",
    "    y=\"USAGE_IN_CURRENCY\",\n",
    "    color=\"RESOURCE_TYPE\",\n",
    "    labels={\"USAGE_IN_CURRENCY\": \"Usage in Currency\", \"USAGE_DATE\": \"Usage Date\"},\n",
    "    title=\"Usage in Currency Over Time\",\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Looking at the usage by cost category, we find virtual warehouses and cloud services are the biggest expenses.\n",
    " We can also clearly visualize cyclical patterns and anomalies. \n",
    "\n",
    "Knowing all that, we can focus our attention to Virtual Warehouses. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metering_query = \"\"\"\n",
    "SELECT ORGANIZATION_NAME,\n",
    "ACCOUNT_NAME,\n",
    "REGION,\n",
    "ACCOUNT_LOCATOR,\n",
    "SERVICE_TYPE,\n",
    "TO_DATE(END_TIME) AS USAGE_DATE,\n",
    "WAREHOUSE_NAME,\n",
    "CREDITS_USED,\n",
    "CREDITS_USED_COMPUTE,\n",
    "CREDITS_USED_CLOUD_SERVICES\n",
    "FROM SNOWFLAKE.ORGANIZATION_USAGE.WAREHOUSE_METERING_HISTORY\n",
    "WHERE WAREHOUSE_ID > 0  -- Skip pseudo-VWs such as \"CLOUD_SERVICES_ONLY\"\n",
    "AND USAGE_DATE BETWEEN '2023-01-01' AND '2023-12-31' -- Only 1 year is available\n",
    "AND ACCOUNT_NAME = 'AA12345' -- Account number\n",
    "\"\"\"\n",
    "cursor.execute(metering_query)\n",
    "metering_df = cursor.fetch_pandas_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metering_df.groupby(\n",
    "    by=[metering_df[\"USAGE_DATE\"].map(lambda x: x.year), \"WAREHOUSE_NAME\"]\n",
    ").agg({\"CREDITS_USED\": \"sum\"}).unstack(level=[1], fill_value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We fill in the missing dates in the dataframe with zeros\n",
    "warehouses = metering_df[\"WAREHOUSE_NAME\"].dropna().unique()\n",
    "dates = pd.date_range(\n",
    "    metering_df[\"USAGE_DATE\"].min(), metering_df[\"USAGE_DATE\"].max(), freq=\"D\"\n",
    ")\n",
    "multi_index = pd.MultiIndex.from_product(\n",
    "    [warehouses, dates], names=[\"WAREHOUSE_NAME\", \"USAGE_DATE\"]\n",
    ")\n",
    "metering_df = (\n",
    "    metering_df.groupby([\"WAREHOUSE_NAME\", \"USAGE_DATE\"])\n",
    "    .agg({\"CREDITS_USED\": \"sum\"})\n",
    "    .reindex(multi_index, fill_value=0)\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "fig = px.line(\n",
    "    metering_df.sort_values(by=\"USAGE_DATE\"),\n",
    "    x=\"USAGE_DATE\",\n",
    "    y=\"CREDITS_USED\",\n",
    "    color=\"WAREHOUSE_NAME\",\n",
    "    labels={\"CREDITS_USED\": \"Credit used\", \"USAGE_DATE\": \"Usage Date\"},\n",
    "    title=\"Credit used by Warehouses Over Time\",\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weekly Metering\n",
    "\n",
    "So far we have only looked at the daily usage for each warehouse. As we can see in the charts, anomalies sometimes persist for more than a day.\n",
    "Let's aggregate the metering data weekly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weekly_metering_df = metering_df[[\"USAGE_DATE\", \"WAREHOUSE_NAME\", \"CREDITS_USED\"]]\n",
    "\n",
    "weekly_metering_df = (\n",
    "    weekly_metering_df.groupby(\n",
    "        [\"WAREHOUSE_NAME\", pd.Grouper(key=\"USAGE_DATE\", freq=\"W\")]\n",
    "    )\n",
    "    .agg({\"CREDITS_USED\": \"sum\"})\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "fig = px.line(\n",
    "    weekly_metering_df.sort_values(by=\"USAGE_DATE\"),\n",
    "    x=\"USAGE_DATE\",\n",
    "    y=\"CREDITS_USED\",\n",
    "    color=\"WAREHOUSE_NAME\",\n",
    "    labels={\"CREDITS_USED\": \"Credit used\", \"USAGE_DATE\": \"Usage Date\"},\n",
    "    title=\"Credit used by Warehouses Over Time\",\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Breakdown by Warehouse\n",
    "\n",
    "Let's look at individual wharehouse to have a better idea of the anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.line(\n",
    "    metering_df.sort_values(by=\"USAGE_DATE\"),\n",
    "    x=\"USAGE_DATE\",\n",
    "    y=\"CREDITS_USED\",\n",
    "    color=\"WAREHOUSE_NAME\",\n",
    "    facet_row=\"WAREHOUSE_NAME\",\n",
    "    labels={\"CREDITS_USED\": \"Credit used\", \"USAGE_DATE\": \"Usage Date\"},\n",
    "    title=\"Credit used by Warehouses Over Time\",\n",
    "    height=2000,\n",
    ")\n",
    "fig.for_each_annotation(lambda a: a.update(text=a.text.split(\"=\")[-1]))\n",
    "fig.update_yaxes(matches=None)\n",
    "fig.update_layout(showlegend=False)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the above chart, we can see some trends and recurring patterns in Warehouse credit usage.\n",
    "\n",
    "Let's apply seasonal_decompose to see if we can isolate seasonality and trend. Outliers will show up in the residuals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import IsolationForest\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "\n",
    "warehouses = metering_df[\"WAREHOUSE_NAME\"].unique()\n",
    "warehouses\n",
    "\n",
    "residuals = {}\n",
    "\n",
    "for warehouse in warehouses:\n",
    "    df = metering_df[metering_df[\"WAREHOUSE_NAME\"] == warehouse]\n",
    "    df.set_index(\"USAGE_DATE\", inplace=True)\n",
    "    compute_stl = seasonal_decompose(\n",
    "        x=df[\"CREDITS_USED\"],\n",
    "        model=\"additive\",\n",
    "        extrapolate_trend=\"freq\",\n",
    "        two_sided=False,\n",
    "    )\n",
    "    compute_stationary = compute_stl.resid.values.reshape(-1, 1)\n",
    "\n",
    "    # Save the residuals to use for anomaly detection\n",
    "    residuals[warehouse] = compute_stationary\n",
    "\n",
    "    fig = compute_stl.plot()\n",
    "    plt.suptitle(warehouse)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for warehouse, compute_stationary in residuals.items():\n",
    "    compute_model = IsolationForest().fit(compute_stationary)\n",
    "\n",
    "    warehouse_filter = metering_df[\"WAREHOUSE_NAME\"] == warehouse\n",
    "    metering_df.loc[\n",
    "        warehouse_filter, \"compute_scores\"\n",
    "    ] = compute_model.decision_function(compute_stationary)\n",
    "\n",
    "    mean_scores = metering_df.loc[warehouse_filter, \"compute_scores\"].mean()\n",
    "    std_scores = metering_df.loc[warehouse_filter, \"compute_scores\"].std()\n",
    "    compute_threshold = mean_scores - (2 * std_scores)\n",
    "\n",
    "    metering_df.loc[warehouse_filter, \"compute_threshold\"] = compute_threshold\n",
    "metering_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anomalies = metering_df.loc[\n",
    "    (metering_df[\"compute_scores\"] <= metering_df[\"compute_threshold\"])\n",
    "    & (metering_df[\"CREDITS_USED\"] > metering_df[\"CREDITS_USED\"].mean())\n",
    "]\n",
    "\n",
    "# Plot the line chart\n",
    "fig = px.line(\n",
    "    metering_df,\n",
    "    x=\"USAGE_DATE\",\n",
    "    y=\"CREDITS_USED\",\n",
    "    color=\"WAREHOUSE_NAME\",\n",
    "    title=\"Credits Used Over Time\",\n",
    "    labels={\"CREDITS_USED\": \"Credits Used\", \"Date_Column\": \"Date\"},\n",
    ")\n",
    "\n",
    "# Highlight data points above the compute_threshold in a different color\n",
    "if not anomalies.empty:\n",
    "    scatter_trace = px.scatter(\n",
    "        anomalies, x=\"USAGE_DATE\", y=\"CREDITS_USED\", color_discrete_sequence=[\"black\"]\n",
    "    )\n",
    "    fig.add_trace(scatter_trace.data[0])\n",
    "# Show the plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anomalies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detecting Drift\n",
    "\n",
    "Evidently Reports are great to quickly test for Data Drift.\n",
    "\n",
    "In our case, we will also use Tests to check drift for each warehouse and trigger retraining of the model when drift is detected. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evidently.report import Report\n",
    "from evidently.metrics import DataDriftTable\n",
    "\n",
    "from evidently.test_suite import TestSuite\n",
    "from evidently.tests import TestAllFeaturesValueDrift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = metering_df[[\"USAGE_DATE\", \"WAREHOUSE_NAME\", \"CREDITS_USED\"]].sort_values(\n",
    "    by=\"USAGE_DATE\"\n",
    ")\n",
    "df = df.pivot(index=\"USAGE_DATE\", columns=\"WAREHOUSE_NAME\", values=\"CREDITS_USED\")\n",
    "df.reset_index(inplace=True)\n",
    "\n",
    "most_recent_week_start = df[\"USAGE_DATE\"].max() - pd.DateOffset(days=7)\n",
    "\n",
    "reference_df = df[df[\"USAGE_DATE\"] < most_recent_week_start]\n",
    "current_df = df[df[\"USAGE_DATE\"] >= most_recent_week_start]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_drift_report = Report(\n",
    "    metrics=[\n",
    "        DataDriftTable(stattest=\"ks\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "data_drift_report.run(current_data=current_df, reference_data=reference_df)\n",
    "data_drift_report.show(mode=\"inline\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reports help visually explore the data or model quality or share results with the team. \n",
    "However, it is less convenient if you want to run your checks automatically and only react to meaningful issues\n",
    "\n",
    "To integrate Evidently checks in the prediction pipeline, you can use the Test Suites functionality. They are also better suited to handle large datasets.\n",
    "\n",
    "Our ultimate goal is to retrain models when drift occurs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "suite = TestSuite(tests=[TestAllFeaturesValueDrift(stattest=\"ks\")])\n",
    "suite.run(reference_data=reference_df, current_data=current_df)\n",
    "suite.show(mode=\"inline\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then use the test results as a dictionnary to automate retraining of the drifted models later on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "pp.pprint(suite.as_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame([x.get(\"parameters\") for x in suite.as_dict().get(\"tests\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
